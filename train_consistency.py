import math
import os
import random
import click
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision # type: ignore
import torchvision.transforms as transforms # type: ignore
from tqdm import tqdm # type: ignore
from toy.toy_utils import calc_karras_sigmas
from unet import Unet
import matplotlib.pyplot as plt
import wandb

from torchvision.transforms import Compose, Resize, Lambda
from torchmetrics.image.fid import FrechetInceptionDistance

@click.command()
@click.option("--device", default="cuda", help="Device to train on")
@click.option("--epochs", default=200, help="Number of epochs to train")
@click.option("--batch-size", default=128, help="Batch size")
@click.option("--lr", default=3e-5, help="Learning rate")
@click.option("--extra-plots", is_flag=True, help="Generate additional training plots", default=True)
@click.option("--enable-wandb", is_flag=True, help="Upload to Wandb", default=False)
@click.option("--model-path", default="model.pt", help="Learning rate")
def train(device, epochs, batch_size, lr, extra_plots, enable_wandb, model_path):
    if enable_wandb:
        wandb.init(
            project="MNIST_consistency_distillation_2",
            entity="actrec",
            config={
                'lr': lr,
            }, 
            name=f"Consistency Training lr:{lr}"
        ) 

    # Setup model and optimizer
    model = Unet(256, 1, 1, base_dim=64, dim_mults=[2, 4]).to(device)
    model_pretrained = None

    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path), strict=False)
        
        # Reset logvar weights for CM
        model.logvar_linear = nn.Linear(model.logvar_linear.in_features, model.logvar_linear.out_features).to(device)
        
        model_pretrained = Unet(256, 1, 1, base_dim=64, dim_mults=[2, 4]).to(device)
        model_pretrained.load_state_dict(torch.load(model_path), strict=False)
        print(f"Loaded model from {model_path}, performing consistency distillation.")
        consistency_training = False
    else:
        print("No model found, performing consistency training.")
        consistency_training = True
        
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2, betas=(0.9, 0.99))
    sigma_data = 0.5
    
    P_mean = -2
    P_std = 1.6
    
    # Load CIFAR-10
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5), (1.0))
    ])
    dataset = torchvision.datasets.MNIST(root='./data', train=True, 
                                         download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, 
                            num_workers=4, pin_memory=True, persistent_workers=True)
            
    step = 0
    for epoch in range(epochs):
        model.train()
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}")
        
        for batch, class_label in progress_bar:
            x0 = batch.to(device)
    
            # Will use this for jvp calcs
            def model_wrapper(scaled_x_t, t):
                pred, logvar = model(scaled_x_t, t.flatten(), return_logvar=True)
                # If you want the model to be conditioned on class label (or anything else), just add it as an additional argument:
                # You do not need to change anything else in the algorithm.
                # pred, logvar = model(scaled_x_t, t.flatten(), class_label, return_logvar=True)
                return pred, logvar
            
            # Sample noise from log-normal distribution
            sigma = torch.randn(x0.shape[0], device=x0.device).reshape(-1, 1, 1, 1)
            sigma = (sigma * P_std + P_mean).exp()  # Sample from proposal distribution
            t = torch.arctan(sigma / sigma_data)  # Convert to t using arctan
            
            # Generate z and x_t
            z = torch.randn_like(x0) * sigma_data
            x_t = torch.cos(t) * x0 + torch.sin(t) * z
            
            if consistency_training:
                # Estimate dx_t/dt (For consistency TRAINING)
                dxt_dt = torch.cos(t) * z - torch.sin(t) * x0
            else:
                # For consistency DISTILLATION
                # (model_pretrained is assumed to output v-predictions)
                with torch.no_grad():
                    pretrain_pred = model_pretrained(x_t / sigma_data, t.flatten())
                    dxt_dt = sigma_data * pretrain_pred
            
            # Next we have to calculate g and F_theta. We can do this simultaneously with torch.func.jvp
            # This doesn't match the paper because I think the paper had a typo
            v_x = torch.cos(t) * torch.sin(t) * dxt_dt / sigma_data
            v_t = torch.cos(t) * torch.sin(t)
            F_theta, F_theta_grad, logvar = torch.func.jvp(
                model_wrapper, 
                (x_t / sigma_data, t),
                (v_x, v_t),
                has_aux=True
            )
            logvar = logvar.view(-1, 1, 1, 1)
            F_theta_grad = F_theta_grad.detach()
            F_theta_minus = F_theta.detach()
            
            # Warmup steps. 10000 was used in the paper. I'm using 1000 for MNIST since it's an easier dataset.
            r = min(1.0, step / 1000)
            # Calculate gradient g using JVP rearrangement
            g = -torch.cos(t) * torch.cos(t) * (sigma_data * F_theta_minus - dxt_dt)
            # Note that F_theta_grad is already multiplied by sin(t) cos(t) from the tangents. Doing it early helps with stability.
            second_term = -r * (torch.cos(t) * torch.sin(t) * x_t + sigma_data * F_theta_grad)
            g = g + second_term
            
            # Tangent normalization
            g_norm = torch.linalg.vector_norm(g, dim=(1, 2, 3), keepdim=True)
            g_norm = g_norm * np.sqrt(g_norm.numel() / g.numel())  # Multiplying by sqrt(numel(g_norm) / numel(g)) ensures that the norm is invariant to the spatial dimensions.
            g = g / (g_norm + 0.1)  # 0.1 is the constant c, can be modified but 0.1 was used in the paper
            
            # Tangent clipping (Only use this OR normalization)
            # g = torch.clamp(g, min=-1, max=1)
            
            # Calculate loss with adaptive weighting
            # Paper uses weight = 1 / sigma, but I've found this to cause instability.
            weight = 1
            loss = (weight / torch.exp(logvar)) * torch.square(F_theta - F_theta_minus - g) + logvar
            loss = loss.mean()
            
            optimizer.zero_grad()
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100.0)
            optimizer.step()

            progress_bar.set_postfix({"loss": loss.item(), "grad_norm": grad_norm.item()})
            step += 1
            
        if enable_wandb:
            wandb.log({'Train_loss': loss.item()}, step=epoch)

        torch.save(model.state_dict(), f'models/consistency/model_consistency_{epoch}.pt')
        
        #
        #
        #
        #
        # Everything below is just for visualization and can be removed safely
        #
        #
        #
        #
        #
        
        if not extra_plots:
            continue
        
        print("Save Sample", end='...')
        z = torch.randn(16, 1, 28, 28, generator=torch.Generator().manual_seed(42)).to(device)
        t0 = 1.56454 * torch.ones(16, device=device)
        t1 = 1.1 * torch.ones(16, device=device)
        with torch.no_grad():
            pred_x0 = torch.clamp(-sigma_data * model(z, t0), min=-0.5, max=0.5)
            
            plt.figure(figsize=(12, 12))
            for i in range(16):
                plt.subplot(4, 4, i+1)
                plt.imshow(pred_x0[i, 0].cpu().numpy(), cmap='gray')
                plt.axis('off')
            plt.tight_layout()
            os.makedirs('outputs_consistency/samples_1step', exist_ok=True)
            plt.savefig(f'outputs_consistency/samples_1step/epoch_{epoch:04d}.png')
            plt.close()
            
            z = torch.randn(16, 1, 28, 28, generator=torch.Generator().manual_seed(43)).to(device)
            t1_exp = t1.view(-1, 1, 1, 1)
            x_t = torch.sin(t1_exp) * z * sigma_data + torch.cos(t1_exp) * pred_x0
            pred_x0 = torch.clamp(torch.cos(t1_exp) * x_t - torch.sin(t1_exp) * sigma_data * model(x_t / sigma_data, t1), min=-0.5, max=0.5)
        
            plt.figure(figsize=(12, 12))
            for i in range(16):
                plt.subplot(4, 4, i+1)
                plt.imshow(pred_x0[i, 0].cpu().numpy(), cmap='gray')
                plt.axis('off')
            plt.tight_layout()
            os.makedirs('outputs_consistency/samples_2step', exist_ok=True)
            plt.savefig(f'outputs_consistency/samples_2step/epoch_{epoch:04d}.png')
            plt.close()
        print("DONE")
        
        #########
        # FID Evaluation
        #########
        print("Compute FID", end="...")
        sample_n = 1_000
        indices = random.sample(range(len(dataset)), sample_n)
        real_images = torch.stack([dataset[i][0] for i in indices])

        z = torch.randn(sample_n, 1, 28, 28, generator=torch.Generator().manual_seed(42)).to(device)
        t0 = 1.56454 * torch.ones(sample_n, device=device)
        t1 = 1.1 * torch.ones(sample_n, device=device)
        with torch.no_grad():
            pred_x0 = torch.clamp(-sigma_data * model(z, t0), min=-0.5, max=0.5)
            generated_imsages = pred_x0
            fid_score_1 = calculate_fid(real_images, generated_imsages, device='cuda')
            
            z = torch.randn(sample_n, 1, 28, 28, generator=torch.Generator().manual_seed(43)).to(device)
            t1_exp = t1.view(-1, 1, 1, 1)
            x_t = torch.sin(t1_exp) * z * sigma_data + torch.cos(t1_exp) * pred_x0
            pred_x0 = torch.clamp(torch.cos(t1_exp) * x_t - torch.sin(t1_exp) * sigma_data * model(x_t / sigma_data, t1), min=-0.5, max=0.5)
            generated_imsages = pred_x0
            fid_score_2 = calculate_fid(real_images, generated_imsages, device='cuda')
            
            print(f"FID score 1: {fid_score_1}, FID score 2: {fid_score_2}")

        if enable_wandb:
            wandb.log({'Train_CD_1_FID': fid_score_1}, step=epoch)
            wandb.log({'Train_CD_2_FID': fid_score_2}, step=epoch)


        # Plot one noise sample with different timesteps
        # Sample one image from the dataset for visualization
        sample_img, _ = dataset[0]
        sample_img = sample_img.unsqueeze(0).to(device)
        
        z = torch.randn(1, 1, 28, 28, generator=torch.Generator().manual_seed(42)).to(device) * sigma_data
        z = z.repeat(16, 1, 1, 1)  # Repeat the same noise 16 times
        t = torch.linspace(0, 1.56454, 16, device=device).view(-1, 1, 1, 1)  # Linearly spaced timesteps
        x_t = torch.cos(t) * sample_img + torch.sin(t) * z
        with torch.no_grad():
            pred_x0 = torch.clamp(torch.cos(t) * x_t - torch.sin(t) * sigma_data * model(x_t / 0.5, t.flatten()), min=-0.5, max=0.5)
            plt.figure(figsize=(12, 12))
            for i in range(16):
                plt.subplot(4, 4, i+1)
                plt.imshow(pred_x0[i, 0].cpu().numpy(), cmap='gray')
                plt.title(f't={t[i].item():.2f}')
                plt.axis('off')
            plt.tight_layout()
            os.makedirs('outputs_consistency/timesteps', exist_ok=True)
            plt.savefig(f'outputs_consistency/timesteps/epoch_{epoch:04d}.png')
            plt.close()
            
        # Testing gradients numerically
        if model_pretrained is not None and epoch % 5 == 0:
            sampling_timesteps = torch.arctan(calc_karras_sigmas(sigma_min=0.002, sigma_max=80, steps=64, rho=7) / sigma_data).to(device)
            trajectory_x0 = []
            trajectory_xt = []
            trajectory_t = []
            trajectory_grad = []
            
            endpoints = torch.randn(2, 1, 28, 28).to(device) * sigma_data
            alphas = torch.linspace(0, 1, 64).view(-1, 1, 1, 1).to(device)
            x_t = (endpoints[0] * alphas + endpoints[1] * (1 - alphas)) / torch.sqrt(alphas**2 + (1 - alphas)**2)
            with torch.no_grad():
                trajectory_guess = -sigma_data * model_wrapper(x_t / sigma_data, sampling_timesteps[0].repeat(x_t.shape[0]))[0]
            
            for t in tqdm(sampling_timesteps):
                trajectory_t.append(t.cpu().numpy())
                trajectory_xt.append(x_t.detach().cpu().numpy())
                
                t_repeated = t.repeat(x_t.shape[0]).view(-1, 1, 1, 1)
            
                with torch.no_grad():
                    dxt_dt = sigma_data * model_pretrained(x_t / sigma_data, t_repeated.flatten())
                
                v_x = torch.cos(t_repeated) * torch.sin(t_repeated) * dxt_dt / sigma_data
                v_t = torch.cos(t_repeated) * torch.sin(t_repeated)
                
                with torch.no_grad():
                    # Calculate JVP using torch.func.jvp
                    F_theta, F_theta_grad, logvar = torch.func.jvp(
                        model_wrapper, 
                        (x_t / sigma_data, t_repeated),
                        (v_x, v_t),
                        has_aux=True
                    )
                    F_theta_grad = F_theta_grad.detach()
                    F_theta_minus = F_theta.detach()
                    
                # Calculate predicted x0
                pred_x0 = torch.cos(t) * x_t - torch.sin(t) * sigma_data * F_theta
                trajectory_x0.append(pred_x0.detach().cpu().numpy())
                
                r = 1
                # Calculate gradient g using JVP rearrangement
                g = -torch.cos(t) * torch.cos(t) * (sigma_data * F_theta_minus - dxt_dt)
                # Note that F_theta_grad is already multiplied by sin(t) cos(t) from the tangents. Doing it early helps with stability.
                second_term = -r * (torch.cos(t) * torch.sin(t) * x_t + sigma_data * F_theta_grad)
                g = g + second_term
                trajectory_grad.append(torch.sqrt(torch.square(g).sum(dim=(1, 2, 3))).cpu().numpy())
                
                next_t = sampling_timesteps[sampling_timesteps < t]
                if len(next_t) == 0:
                    next_t = 0
                else:
                    next_t = next_t[0]
                
                # Calculate x0 prediction and update x
                x_t = torch.cos(t - next_t) * x_t - torch.sin(t - next_t) * dxt_dt
                x_t = x_t.detach()
                
            trajectory_x0 = np.array(trajectory_x0)
            trajectory_xt = np.array(trajectory_xt)
            trajectory_t = np.array(trajectory_t)
            trajectory_grad = np.array(trajectory_grad)
                
            diff = torch.abs(x_t - trajectory_guess).mean().item()
            print(f"Consistency error: {diff:.4f}")
            if enable_wandb:
                wandb.log({'consistency error (val)': round(diff, 4)}, step=epoch)
            discrete_grad = np.zeros_like(trajectory_x0)
            discrete_grad[:-1] = -np.cos(trajectory_t[:-1])[:, None, None, None, None] * (trajectory_x0[1:] - trajectory_x0[:-1]) / (trajectory_t[1:] - trajectory_t[:-1])[:, None, None, None, None]
            discrete_grad = np.sqrt(np.sum(np.square(discrete_grad), axis=(2, 3, 4)))

            # Create figure with two subplots side by side
            plt.figure(figsize=(12, 4))

            # Get shared vmin/vmax for consistent scale
            vmin = min(trajectory_grad.min(), discrete_grad.min())
            vmax = max(trajectory_grad.max(), discrete_grad.max())

            # Plot trajectory gradient 
            plt.subplot(121)
            plt.imshow(trajectory_grad, aspect='auto', cmap='RdBu', 
                      extent=[0, trajectory_grad.shape[1], 0, math.pi/2],
                      vmin=vmin, vmax=vmax)
            plt.colorbar(label='Gradient magnitude')
            plt.title('Trajectory Gradient')
            plt.xlabel('Initial X')
            plt.ylabel('Time step')
            
            # Plot discrete gradient
            plt.subplot(122)
            plt.imshow(discrete_grad, aspect='auto', cmap='RdBu',
                      extent=[0, discrete_grad.shape[1], 0, math.pi/2],
                      vmin=vmin, vmax=vmax)
            plt.colorbar(label='Gradient magnitude')
            plt.title('Discrete Gradient')
            plt.xlabel('Initial X')
            plt.ylabel('Time step')

            plt.tight_layout()
            os.makedirs('outputs_consistency/gradients', exist_ok=True)
            plt.savefig(f'outputs_consistency/gradients/epoch_{epoch:04d}.png')
            plt.close()
    
    wandb.finish()

def calculate_fid(real_images, generated_images, device='cuda'):
    """
    Calculates FID between real and generated MNIST images.
    
    Args:
        real_images (Tensor): Real images (N, 1, H, W), values in [0, 1] or [-1, 1].
        generated_images (Tensor): Generated images (N, 1, H, W), same range as real.
        device (str): 'cuda' or 'cpu'.
        
    Returns:
        float: FID score.
    """
    # Convert to 3 channels and resize to 299x299 for InceptionV3
    transform = Compose([
        Lambda(lambda x: (x + 1) / 2 if x.min() < 0 else x),  # Rescale to [0, 1] if needed
        Lambda(lambda x: x.repeat(3, 1, 1)),  # (1, H, W) -> (3, H, W)
        Resize((299, 299))
    ])
    
    real_images = real_images.to(device)
    generated_images = generated_images.to(device)

    fid = FrechetInceptionDistance(feature=2048).to(device)

    with torch.no_grad():
        for img in real_images:
            img = transform(img)
            img = img.clamp(0, 1) * 255  # Ensure values in [0, 255]
            img = img.to(torch.uint8)
            fid.update(img.unsqueeze(0), real=True)

        for img in generated_images:
            img = transform(img)
            img = img.clamp(0, 1) * 255  # Ensure values in [0, 255]
            img = img.to(torch.uint8)
            fid.update(img.unsqueeze(0), real=False)

    return fid.compute().item()

if __name__ == '__main__':
    train()